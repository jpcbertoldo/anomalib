{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models\n",
    "\n",
    "Two models (PaDiM and PatchCore) will be trained and evaluated with a per-image metric (`AULogPImO`).\n",
    "\n",
    "See the notebook `502a_perimg_metrics.ipynb` to firs get familiarized with this metric.\n",
    "\n",
    "The model performances are exported and loaded back, then the two models are compared based on each image's scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add link to notebook mentioned above\n",
    "# TODO pick model params to make them more competitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Anomalib\n",
    "\n",
    "The easiest way to install anomalib is to use pip. You can install it from the command line using the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anomalib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a cell print all the outputs instead of just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use MVTec AD DataModule. \n",
    "\n",
    "> See [these notebooks](https://github.com/openvinotoolkit/anomalib/tree/main/notebooks/100_datamodules) for more details on datamodules. \n",
    "\n",
    "We assume that `datasets` directory is created in the `anomalib` root directory and `MVTec` dataset is located in `datasets` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# NOTE: Provide the path to the dataset root directory.\n",
    "#   If the datasets is not downloaded, it will be downloaded to this directory.\n",
    "dataset_root = Path.cwd().parent.parent.parent / \"datasets\" / \"MVTec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working on a segmentation task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.data import TaskType\n",
    "\n",
    "task = TaskType.SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the `hazelnut` category at resolution of 256x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.data.mvtec import MVTec\n",
    "\n",
    "datamodule = MVTec(\n",
    "    root=dataset_root,\n",
    "    category=\"hazelnut\",\n",
    "    image_size=256,\n",
    "    train_batch_size=32,\n",
    "    eval_batch_size=32,\n",
    "    num_workers=8,\n",
    "    task=task,\n",
    ")\n",
    "datamodule.setup()\n",
    "i, data = next(enumerate(datamodule.test_dataloader()))\n",
    "print(f'Image Shape: {data[\"image\"].shape} Mask Shape: {data[\"mask\"].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "We train two models: PaDiM and PatchCore.\n",
    "\n",
    "> See [these notebooks](https://github.com/openvinotoolkit/anomalib/tree/main/notebooks/200_models) for more details on models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaDiM\n",
    "\n",
    "The next cell instantiates and trains PaDiM.\n",
    "\n",
    "The `MetricsConfigurationCallback()` will not have metric because they will be created manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from anomalib.utils.callbacks import MetricsConfigurationCallback, PostProcessingConfigurationCallback\n",
    "from anomalib.post_processing import NormalizationMethod, ThresholdMethod\n",
    "from anomalib.models import Padim\n",
    "\n",
    "padim = Padim(\n",
    "    input_size=(256, 256),\n",
    "    layers=[\n",
    "        \"layer1\",\n",
    "        \"layer2\",\n",
    "    ],\n",
    "    backbone=\"resnet18\",\n",
    "    pre_trained=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[\n",
    "        PostProcessingConfigurationCallback(\n",
    "            normalization_method=NormalizationMethod.MIN_MAX,\n",
    "            threshold_method=ThresholdMethod.ADAPTIVE,\n",
    "        ),\n",
    "        MetricsConfigurationCallback(),\n",
    "    ],\n",
    "    max_epochs=1,\n",
    "    num_sanity_val_steps=0,  # does not work for padim\n",
    "    accelerator=\"auto\",\n",
    ")\n",
    "\n",
    "trainer.fit(datamodule=datamodule, model=padim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PatchCore\n",
    "\n",
    "The next cell instantiates and trains PatchCore.\n",
    "\n",
    "The `MetricsConfigurationCallback()` will not have metric because they will be created manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from anomalib.post_processing import ThresholdMethod\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from anomalib.models import Patchcore\n",
    "\n",
    "patchcore = Patchcore(\n",
    "    input_size=(256, 256),\n",
    "    backbone=\"resnet18\",\n",
    "    layers=[\n",
    "        \"layer3\",\n",
    "    ],\n",
    "    coreset_sampling_ratio=0.1,\n",
    "    num_neighbors=9,\n",
    "    pre_trained=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[\n",
    "        PostProcessingConfigurationCallback(\n",
    "            normalization_method=NormalizationMethod.MIN_MAX,\n",
    "            threshold_method=ThresholdMethod.ADAPTIVE,\n",
    "        ),\n",
    "        MetricsConfigurationCallback(),\n",
    "    ],\n",
    "    max_epochs=1,\n",
    "    num_sanity_val_steps=0,  # does not work for patchcore\n",
    "    accelerator=\"auto\",\n",
    ")\n",
    "\n",
    "trainer.fit(datamodule=datamodule, model=patchcore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process test images\n",
    "\n",
    "This part is usually happening automatically but here we want to extract the outputs manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "_ = padim.eval()\n",
    "_ = patchcore.eval()\n",
    "\n",
    "anomaly_maps_padim = []\n",
    "anomaly_maps_patchcore = []\n",
    "masks = []\n",
    "\n",
    "for batchidx, batch in enumerate(datamodule.test_dataloader()):\n",
    "    anomaly_maps_padim.append(padim.test_step_end(padim.test_step(batch, batchidx))[\"anomaly_maps\"].squeeze(1))\n",
    "    anomaly_maps_patchcore.append(\n",
    "        patchcore.test_step_end(patchcore.test_step(batch, batchidx))[\"anomaly_maps\"].squeeze(1)\n",
    "    )\n",
    "    masks.append(padim.test_step_end(padim.test_step(batch, batchidx))[\"mask\"].int())\n",
    "\n",
    "anomaly_maps_padim = torch.cat(anomaly_maps_padim)\n",
    "anomaly_maps_patchcore = torch.cat(anomaly_maps_patchcore)\n",
    "masks = torch.cat(masks)\n",
    "\n",
    "print(f\"{anomaly_maps_padim.shape=} {anomaly_maps_patchcore.shape=} {masks.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AULogPImO`\n",
    "\n",
    "> Area Under the Log Per-Image Overlap curve; pronounced \"a-u-log-pee-mo\"\n",
    "\n",
    "Reminder of the metric definition.\n",
    "\n",
    "The `LogPImO` curves have a shared X-axis and a per-image Y-axis.\n",
    "\n",
    "The X-axis (\"shared-FPR\"):\n",
    "- is a metric of False Positives only in the normal images (here it is the log10 of the mean of in-image FPRs, which equals the set-FPR)\n",
    "- is shared by all anomalous images/curves\n",
    "\n",
    "The Y-axis: \n",
    "- is, at a given threshold (index by the X-axis), the **overlap** between the binary predicted mask and the ground truth mask, which corresponds to the in-image TPR\n",
    "- has one value per image, so there is one LogPImO curve per image. \n",
    "\n",
    "> - FPR: False Positive Rate\n",
    "> - TPR: True Positive Rate\n",
    "\n",
    "The `AULogPImO` $\\in [0, 1]$ is the area under each of these curves normalized by the maximum possible area (when TPR is constant at 1 for all FPR values):\n",
    "\n",
    "$$\n",
    "    \\frac{\\int_{L}^{U} \\; \\text{TPR}( \\, \\text{FPR} \\, ) \\; d\\text{log(FPR)}}{log(U/L)} \n",
    "    \\quad ,\n",
    "    \n",
    "$$\n",
    "\n",
    "where $L \\in (0, 1)$ is the shared-FPR lower bound, and $U \\in (0, 1]$ is the shared-FPR upper bound such that $U > L$.\n",
    "\n",
    "> **Score of a random model**\n",
    "> \n",
    "> The `AULogPImO` score of a random model (i.e. $\\text{TPR}(\\text{FPR}) = \\text{FPR}$) is \n",
    ">    \n",
    "> $$\n",
    "> \\frac{U - L}{log(U / L)}\n",
    "> \\quad .\n",
    "> $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaDiM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import scipy as sp\n",
    "from anomalib.utils.metrics.perimg import AULogPImO\n",
    "\n",
    "aulogpimo_padim = AULogPImO(lbound=0.001, ubound=0.03)\n",
    "print(f\"AULogPImO of a random model: {aulogpimo_padim.random_model_auc:.1%}\")\n",
    "aulogpimo_padim.cpu()\n",
    "aulogpimo_padim.update(anomaly_maps_padim, masks)\n",
    "pimoresult_padim, aucs_padim = aulogpimo_padim.compute()\n",
    "print(sp.stats.describe(aucs_padim[~torch.isnan(aucs_padim)]))  # `~torch.isnan(aucs)` is removing the `nan`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = aulogpimo_padim.plot()\n",
    "_ = fig.suptitle(\"AULogPImO of PADIM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PatchCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import scipy as sp\n",
    "from anomalib.utils.metrics.perimg import AULogPImO\n",
    "\n",
    "aulogpimo_patchcore = AULogPImO(lbound=0.001, ubound=0.03)\n",
    "print(f\"AULogPImO of a random model: {aulogpimo_patchcore.random_model_auc:.1%}\")\n",
    "aulogpimo_patchcore.cpu()\n",
    "aulogpimo_patchcore.update(anomaly_maps_patchcore, masks)\n",
    "pimoresult_patchcore, aucs_patchcore = aulogpimo_patchcore.compute()\n",
    "print(sp.stats.describe(aucs_patchcore[~torch.isnan(aucs_patchcore)]))  # `~torch.isnan(aucs)` is removing the `nan`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = aulogpimo_patchcore.plot()\n",
    "_ = fig.suptitle(\"AULogPImO of PatchCore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "\n",
    "Save the AUCs from both models in the disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CACHE = Path() / \".cache\" / \"502b\"\n",
    "CACHE.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aulogpimo_padim.save(CACHE / \"aulogpimo_padim.json\")\n",
    "aulogpimo_patchcore.save(CACHE / \"aulogpimo_patchcore.json\")\n",
    "\n",
    "%ls -lh $CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aulogpimo_patchcore, pimoresult_patchcore, aucs_patchcore\n",
    "del aulogpimo_padim, pimoresult_padim, aucs_padim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "Reload back from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.utils.metrics.perimg import AULogPImO\n",
    "\n",
    "pimoresult_padim, aucs_padim = AULogPImO.load(CACHE / \"aulogpimo_padim.json\")\n",
    "lbound, ubound = aucs_padim[\"lbound\"], aucs_padim[\"ubound\"]  # they are the same for both models\n",
    "aucs_padim = aucs_padim[\"aulogpimo\"]\n",
    "\n",
    "pimoresult_patchcore, aucs_patchcore = AULogPImO.load(CACHE / \"aulogpimo_patchcore.json\")\n",
    "aucs_patchcore = aucs_patchcore[\"aulogpimo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# the funcional interface of the plotting funcions have to be used because the\n",
    "# aulogpimo object is gone\n",
    "AULogPImO\n",
    "from anomalib.utils.metrics.perimg.pimo import AULogPImO\n",
    "from anomalib.utils.metrics.perimg.plot import plot_boxplot_logpimo_curves, plot_aulogpimo_boxplot\n",
    "from anomalib.utils.metrics.perimg.common import perimg_boxplot_stats\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 13))\n",
    "\n",
    "for axrow, pimoresult, aucs, model in zip(\n",
    "    axes, [pimoresult_padim, pimoresult_patchcore], [aucs_padim, aucs_patchcore], [\"PaDiM\", \"PatchCore\"]\n",
    "):\n",
    "    _ = plot_aulogpimo_boxplot(\n",
    "        aucs,\n",
    "        pimoresult.image_classes,\n",
    "        # optional\n",
    "        random_model_auc=AULogPImO.random_model_auc_from_bounds(lbound, ubound),\n",
    "        ax=axrow[0],\n",
    "    )\n",
    "    _ = axrow[0].set_title(f\"{model} - boxplot\")\n",
    "\n",
    "    bp_stats = perimg_boxplot_stats(aucs, pimoresult.image_classes, only_class=1)\n",
    "\n",
    "    _ = plot_boxplot_logpimo_curves(\n",
    "        pimoresult.shared_fpr,\n",
    "        pimoresult.tprs,\n",
    "        pimoresult.image_classes,\n",
    "        bp_stats,\n",
    "        lbound,\n",
    "        ubound,\n",
    "        ax=axrow[1],\n",
    "    )\n",
    "    _ = axrow[1].set_title(f\"{model} - curves\")\n",
    "\n",
    "minmin = min([ax.get_xlim()[0] for ax in axes[:, 0].flatten()])\n",
    "maxmax = max([ax.get_xlim()[1] for ax in axes[:, 0].flatten()])\n",
    "for ax in axes[:, 0].flatten():\n",
    "    _ = ax.set_xlim(minmin, maxmax)\n",
    "\n",
    "fig.suptitle(\"AULogPImO: PaDiM vs PatchCore\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image by image comparison (`PaDiM` vs `PatchCore`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric (metric comparison)\n",
    "\n",
    "Here we compare models directly with the value of `AULogPImO`.\n",
    "\n",
    "We are mostly interested in doing a statistical test (presented in a table), and a plot illustrates the used in the test.\n",
    "\n",
    "Plot: \n",
    "- X-axis: image index\n",
    "- Y-axis: its metric value (i.e. `AULogPImO` here)\n",
    "- Different colors/markers represent different models\n",
    "- Horizontal dashed lines: per-model averages (over all images)\n",
    "- The score of a theoretical random model is added for reference\n",
    "\n",
    "Table (statistical test):\n",
    "- Rows (`model1`) and columns (`model2`): models sorted by average score (dashed lines in the plot).\n",
    "- Cells: confidence level that `model1` is better than `model2` [test on the expected values].\n",
    "- Dependent t-test for paired samples is used.\n",
    "\n",
    "\n",
    "> **Confidence Level**\n",
    "> \n",
    "> The null hypothesis is that `mean(model1) == mean(model2)`.\n",
    "> \n",
    "> So the \"confidence level\" is the \"confidence level [that the null hypothesis is false]\" ` = 1 - pvalue`.\n",
    "> \n",
    "> *Higher confidence level means* \"more confident that `mean(model1) > mean(model2)`.\n",
    "\n",
    "\n",
    "> **Dependent t-test for paired samples**\n",
    "> \n",
    "> The statistical test here is a paired T-test with \n",
    "> - null hypothesis: `average_performance(model1) == average_performance(model2)` \n",
    "> - alternative hypothesis: `average_performance(model1) > average_performance(model2)`.\n",
    ">\n",
    "> References: \n",
    "> - [`scipy.stats.ttest_rel`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) \n",
    "> - [\"Dependent t-test for paired samples\" at Wikipedia's page on \"Student's t-test\"](https://en.wikipedia.org/wiki/Student's_t-test#Dependent_t-test_for_paired_samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.utils.metrics.perimg.pimo import AULogPImO\n",
    "from anomalib.utils.metrics.perimg.plot import compare_models_perimg\n",
    "from anomalib.utils.metrics.perimg.common import compare_models_parametric\n",
    "\n",
    "models = {\n",
    "    \"PaDiM\": aucs_padim,\n",
    "    \"PatchCore\": aucs_patchcore,\n",
    "}\n",
    "\n",
    "compare_models_perimg(\n",
    "    models,\n",
    "    \"AULogPImO\",\n",
    "    random_model_score=AULogPImO.random_model_auc_from_bounds(lbound, ubound),\n",
    ")\n",
    "\n",
    "compare_models_parametric(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric (rank comparison)\n",
    "\n",
    "The non-parametric comparison only considers the rank of the models at each image.\n",
    "\n",
    "In other words, Dependent t-test for paired samples\n",
    "\n",
    "Plot: \n",
    "- X-axis: image index\n",
    "- Y-axis: rank\n",
    "- Different colors/markers represent different models\n",
    "- Horizontal dashed lines: per-model average rank (over all images)\n",
    "- A red line between two ranks is added when the metric values are very close (absolute difference is within a tolerance)\n",
    "\n",
    "Table (statistical test):\n",
    "- Rows (`model1`) and columns (`model2`): models sorted by average rank (dashed lines in the plot).\n",
    "- Cells: confidence level that `model1` is better than `model2`\n",
    "- Wilcoxon signed rank test is used.\n",
    "\n",
    "\n",
    "> **Rank**\n",
    "> \n",
    "> Ranks are values between 1 and `num_models` (1 is the best).\n",
    ">\n",
    "> At each image, the models are sorted and their position in the sorting is assigned to them.\n",
    "> \n",
    "> In other words, only the relative position between the models is considered; the metric value itself is \"discarded\".\n",
    ">\n",
    "> When two models are tied (same metric value), the average between their (should-be) ranks is taken. \n",
    "> Ex: if 1st and 2nd best models are tied, then 1.5 is assined to both.\n",
    "\n",
    "\n",
    "> **Wilcoxon signed rank test**\n",
    ">\n",
    "> This test is a non-parametric equivalent of the *Dependent t-test for paired samples*\n",
    "> \n",
    "> Null and alternative hypothesis are the same (alt.: `average_performance(model1) > average_performance(model2)`). \n",
    ">\n",
    "> References: \n",
    "> - [`scipy.stats.wilcoxon`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html#scipy.stats.wilcoxon) \n",
    "> - [\"Wilcoxon signed-rank test\" in Wikipedia](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.utils.metrics.perimg.pimo import AULogPImO\n",
    "from anomalib.utils.metrics.perimg.plot import compare_models_perimg_rank\n",
    "from anomalib.utils.metrics.perimg.common import compare_models_nonparametric\n",
    "\n",
    "models = {\n",
    "    \"PaDiM\": aucs_padim,\n",
    "    \"PatchCore\": aucs_patchcore,\n",
    "}\n",
    "compare_models_perimg_rank(models, \"AULogPImO\")\n",
    "compare_models_nonparametric(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a new model\n",
    "\n",
    "Now we execute the same train-eval process from before using FastFlow.\n",
    "\n",
    "Then, we will redo the same comparisons above but with all the three models at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastFlow\n",
    "\n",
    "Simplified from [`notebooks/200_models/201_fastflow.ipynb`](https://github.com/openvinotoolkit/anomalib/blob/main/notebooks/200_models/201_fastflow.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from functools import partial, update_wrapper\n",
    "from types import MethodType\n",
    "from anomalib.post_processing import ThresholdMethod\n",
    "from pytorch_lightning import Trainer\n",
    "from anomalib.models import Fastflow\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch.optim import Optimizer, Adam\n",
    "\n",
    "fastflow = Fastflow(\n",
    "    input_size=(256, 256),\n",
    "    backbone=\"resnet18\",\n",
    "    pre_trained=True,\n",
    "    flow_steps=8,\n",
    "    hidden_ratio=1.0,\n",
    "    conv3x3_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def configure_optimizers(lightning_module: LightningModule, optimizer: Optimizer):\n",
    "    \"\"\"Override to customize the LightningModule.configure_optimizers` method.\"\"\"\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "fn = partial(\n",
    "    configure_optimizers, optimizer=Adam(params=fastflow.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-5)\n",
    ")\n",
    "update_wrapper(fn, configure_optimizers)  # necessary for `is_overridden`\n",
    "fastflow.configure_optimizers = MethodType(fn, fastflow)\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[\n",
    "        PostProcessingConfigurationCallback(\n",
    "            normalization_method=NormalizationMethod.MIN_MAX,\n",
    "            threshold_method=ThresholdMethod.ADAPTIVE,\n",
    "        ),\n",
    "        MetricsConfigurationCallback(\n",
    "            task=TaskType.SEGMENTATION,\n",
    "            pixel_metrics=[\n",
    "                \"AUROC\",\n",
    "            ],\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            mode=\"max\",\n",
    "            monitor=\"pixel_AUROC\",\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"pixel_AUROC\",\n",
    "            mode=\"max\",\n",
    "            patience=3,\n",
    "        ),\n",
    "    ],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "trainer.fit(datamodule=datamodule, model=fastflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import scipy as sp\n",
    "from anomalib.utils.metrics.perimg import AULogPImO\n",
    "import torch\n",
    "\n",
    "_ = fastflow.eval()\n",
    "\n",
    "anomaly_maps_fastflow = []\n",
    "\n",
    "for batchidx, batch in enumerate(datamodule.test_dataloader()):\n",
    "    anomaly_maps_fastflow.append(fastflow.test_step_end(fastflow.test_step(batch, batchidx))[\"anomaly_maps\"].squeeze(1))\n",
    "\n",
    "anomaly_maps_fastflow = torch.cat(anomaly_maps_fastflow)\n",
    "print(f\"{anomaly_maps_fastflow.shape=}\")\n",
    "\n",
    "aulogpimo_fastflow = AULogPImO(lbound=0.001, ubound=0.03)\n",
    "aulogpimo_fastflow.cpu()\n",
    "aulogpimo_fastflow.update(anomaly_maps_fastflow, masks)\n",
    "\n",
    "fig, axes = aulogpimo_fastflow.plot()\n",
    "_ = fig.suptitle(\"AULogPImO of FastFlow\")\n",
    "\n",
    "_ = aulogpimo_fastflow.save(CACHE / \"aulogpimo_fastflow.json\")\n",
    "%ls -lh $CACHE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pimoresult_fastflow, aucs_fastflow = AULogPImO.load(CACHE / \"aulogpimo_fastflow.json\")\n",
    "aucs_fastflow = aucs_fastflow[\"aulogpimo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image by image comparison (all vs all)\n",
    "\n",
    "The statistical tests are done pairwise.\n",
    "\n",
    "Each cell in the tables show the confidence that `model1` (row) is better than `model2` (column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg.pimo import AULogPImO\n",
    "from anomalib.utils.metrics.perimg.plot import compare_models_perimg_rank, compare_models_perimg\n",
    "from anomalib.utils.metrics.perimg.common import compare_models_nonparametric, compare_models_parametric\n",
    "\n",
    "models = {\n",
    "    \"PaDiM\": aucs_padim,\n",
    "    \"PatchCore\": aucs_patchcore,\n",
    "    \"FastFlow\": aucs_fastflow,\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 13))\n",
    "\n",
    "_ = compare_models_perimg(models, \"AULogPImO\", ax=axes[0])\n",
    "_ = compare_models_perimg_rank(models, \"AULogPImO\", ax=axes[1])\n",
    "fig.suptitle(\"AULogPImO: PaDiM vs PatchCore vs FastFlow\", fontsize=20)\n",
    "\n",
    "print(\"Parametric tests:\")\n",
    "compare_models_parametric(models)\n",
    "\n",
    "print(\"Non-parametric tests:\")\n",
    "compare_models_nonparametric(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to the benchmark\n",
    "\n",
    "coming soon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib-dev-gsoc",
   "language": "python",
   "name": "anomalib-dev-gsoc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
