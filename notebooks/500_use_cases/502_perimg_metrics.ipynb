{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Image Metrics\n",
    "\n",
    "How to use, plot, and compare models using per-image [pixel-wise] metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Anomalib\n",
    "\n",
    "The easiest way to install anomalib is to use pip. You can install it from the command line using the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anomalib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a cell print all the outputs instead of just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use MVTec AD DataModule. \n",
    "\n",
    "> See [these notebooks](https://github.com/openvinotoolkit/anomalib/tree/main/notebooks/100_datamodules) for more details on datamodules. \n",
    "\n",
    "We assume that `datasets` directory is created in the `anomalib` root directory and `MVTec` dataset is located in `datasets` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# NOTE: Provide the path to the dataset root directory.\n",
    "#   If the datasets is not downloaded, it will be downloaded to this directory.\n",
    "dataset_root = Path.cwd().parent.parent / \"datasets\" / \"MVTec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working on a segmentation task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.data import TaskType\n",
    "\n",
    "task = TaskType.SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the `hazelnut` category at resolution of 256x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.data.mvtec import MVTec\n",
    "\n",
    "datamodule = MVTec(\n",
    "    root=dataset_root,\n",
    "    category=\"hazelnut\",\n",
    "    image_size=256,\n",
    "    train_batch_size=32,\n",
    "    eval_batch_size=32,\n",
    "    num_workers=8,\n",
    "    task=task,\n",
    ")\n",
    "datamodule.setup()\n",
    "i, data = next(enumerate(datamodule.test_dataloader()))\n",
    "print(f'Image Shape: {data[\"image\"].shape} Mask Shape: {data[\"mask\"].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We will use PaDiM.\n",
    "\n",
    "> See [these notebooks](https://github.com/openvinotoolkit/anomalib/tree/main/notebooks/200_models) for more details on models. \n",
    "\n",
    "The next cell instantiates and trains the model.\n",
    "\n",
    "The `MetricsConfigurationCallback()` will not have metric because they will be created manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from anomalib.utils.callbacks import MetricsConfigurationCallback, PostProcessingConfigurationCallback\n",
    "from anomalib.post_processing import NormalizationMethod, ThresholdMethod\n",
    "from anomalib.models import Padim\n",
    "\n",
    "model = Padim(\n",
    "    input_size=(256, 256),\n",
    "    layers=[\n",
    "        \"layer1\",\n",
    "        \"layer2\",\n",
    "    ],\n",
    "    backbone=\"resnet18\",\n",
    "    pre_trained=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[\n",
    "        PostProcessingConfigurationCallback(\n",
    "            normalization_method=NormalizationMethod.MIN_MAX,\n",
    "            threshold_method=ThresholdMethod.ADAPTIVE,\n",
    "        ),\n",
    "        MetricsConfigurationCallback(),\n",
    "    ],\n",
    "    max_epochs=1,\n",
    "    num_sanity_val_steps=0,  # does not work for padim\n",
    "    accelerator=\"auto\",\n",
    ")\n",
    "\n",
    "trainer.fit(datamodule=datamodule, model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process test images\n",
    "\n",
    "This part is usually happening automatically but here we want to extract the outputs manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "outputs = []\n",
    "for batchidx, batch in enumerate(datamodule.test_dataloader()):\n",
    "    outputs.append(model.test_step_end(model.test_step(batch, batchidx)))\n",
    "\n",
    "anomaly_maps = torch.squeeze(torch.cat([o[\"anomaly_maps\"] for o in outputs], dim=0))\n",
    "masks = torch.squeeze(torch.cat([o[\"mask\"] for o in outputs], dim=0)).int()\n",
    "print(f\"{anomaly_maps.shape=} {masks.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel-wise [set] metrics\n",
    "\n",
    "The usual set pixel-wise metrics. \n",
    "\n",
    "Only one value for the whole test set is measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from anomalib.utils.metrics import AUROC, AUPR\n",
    "\n",
    "metrics = [AUROC(), AUPR()]\n",
    "\n",
    "for metric in metrics:\n",
    "    metric.cpu()\n",
    "    metric.update(anomaly_maps, masks)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"{metric}={metric.compute()}\")\n",
    "    metric.generate_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AUPImO` (init, update, compute)\n",
    "\n",
    "Area Under the Per-Image Overlap (`AUPImO`) \n",
    "\n",
    "Let's instantiate, load the data, then compute PImO curves and their AUCs (AUPImO scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%autoreload 2\n",
    "\n",
    "from anomalib.utils.metrics.perimg import AUPImO\n",
    "\n",
    "aupimo = AUPImO()\n",
    "aupimo.cpu()\n",
    "aupimo.update(anomaly_maps, masks)\n",
    "\n",
    "pimoresult, aucs = aupimo.compute()\n",
    "(thresholds, fprs, shared_fpr, tprs, image_classes) = pimoresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pimoresult?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{thresholds.shape=}\")\n",
    "print(f\"{fprs.shape=}\")\n",
    "print(f\"{shared_fpr.shape=}\")\n",
    "print(f\"{tprs.shape=}\")\n",
    "print(f\"{image_classes.shape=}\")\n",
    "print(f\"{aucs.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `PImO` curves (plot)\n",
    "\n",
    "> Pronounced \"pee-mo\"\n",
    "\n",
    "The PImO curve has a shared X-axis and a per-image Y-axis.\n",
    "\n",
    "The X-axis:\n",
    "- is a metric of False Positives only in the normal images (here it is the set-FPR)\n",
    "- is shared by all image instances\n",
    "\n",
    "The Y-axis: \n",
    "- is the **overlap** between the binary predicted mask and the ground truth mask, which corresponds to the True Positive Rate (TPR) in a single image\n",
    "- has one value per image, so there is one PImO curve per image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aupimo.plot_all_pimo_curves()\n",
    "# TODO add functional interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AUPImO` = AUC(`PImO`)\n",
    "\n",
    "> Pronounced \"a-u-pee-mo\"\n",
    "\n",
    "The Area Under the Curve (AUC) is, by consequence, computed for each image, which will be used as is score.\n",
    "\n",
    "Notice that `aucs` has the number of images seen in `outputs` (cf. `masks` below).\n",
    "\n",
    "`aucs` has `nan` values for the normal images because the `Per-Image Overlap`, by definition, is not defined on them (they do not have any positive/anomalous pixels).\n",
    "\n",
    "This is done by design choice so the indexes in `aucs` correspond to the indices of the actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{masks.shape[0]=}  ==  {aucs.shape[0]=}\")\n",
    "print(aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AUPImO` distribuion (boxplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can now analyze the distribution of this True Posivity metric across images and take statistics from the test set (e.g. with `sp.stats.describe`).\n",
    "\n",
    "`AUPImO` has an integrated feature to plot a boxplot from the distribution and inspect representative cases using its statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "print(sp.stats.describe(aucs[~torch.isnan(aucs)]))  # `~torch.isnan(aucs)` is removing the `nan`s\n",
    "aupimo.plot_boxplot()\n",
    "aupimo.boxplot_stats()[-3:]\n",
    "# TODO add functional interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representative samples (curve + boxplot)\n",
    "\n",
    "The two plots (`PImO` curve + `AUPImO` boxplot) are combined with the method `AUPImO.plot()`.\n",
    "\n",
    "The `PImO` curves are plot only for the samples that correspond to the boxplot's statistics (see `AUPImO.boxplot_stats()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aupimo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AUPImO` with restricted FPR\n",
    "\n",
    "Notice, from the boxplot, that most images (>75\\%) have an AUC value above 99\\%, which makes sense from the curves because they all seem quite close to the upper left corner of the plot.\n",
    "However, this has a negative effect making the difference between images nearly indistinguishable.\n",
    "\n",
    "Here we apply a restriction to the choice of operating point (anomaly score threshold) to make the metric harder and avoid that issue.\n",
    "\n",
    "---\n",
    "\n",
    "The AUC of a curve with the x-axis in [0, 1] can be interpreted as the average value of the y-axis:\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{AUPImO} = \\int_{0}^1 y( x ) dx = \\frac{\\int_{0}^1 y( x ) dx }{\\int_{0}^1 dx}\n",
    "\n",
    "$$\n",
    "\n",
    "where $x$ is the FPR metric and $y(x)$ is the TPR metric indexed by the former.\n",
    "\n",
    "So the `AUPImO`, like `AUROC` and `AUPRO`, is a true positive metric *averaged* over operating points (again, indexed by another metric, the FPR).\n",
    "\n",
    "Since a large part of the curves are close to `PImO == 100%` (i.e. `TPR == 100%`) for values of of `FPR >= 30%`, the AUC (i.e. the average) is pushed towards 100\\%.\n",
    "\n",
    "However, the right-most end of the curve coresponds to operating points with high FPR, which is, by definition of anomaly detection, undisarable and should be avoided.\n",
    "\n",
    "Therefore we can disconsider this region and only average the curve up to a maximum (tolerated) FPR. \n",
    "\n",
    "---\n",
    "\n",
    "We add a hard requirement the FPR metric with upper bound of FPR range (`AUPImO.ubound`).\n",
    "The AUC is accordingly normalized by the size of the integration range so that it is scaled in [0, 1]:\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{AUPImO} = \\frac{\\int_{0}^{\\texttt{ubound}} y( x ) dx }{\\int_{0}^{\\texttt{ubound}} dx} = \\frac{\\int_{0}^{\\texttt{ubound}} y( x ) dx }{\\texttt{ubound}}\n",
    "\n",
    "$$\n",
    "\n",
    "> Important property: the shared FPR (X-axis) only depends on normal images, so the collection of anomalies available to test the model does **not** affect the model requirement and, therefore, the integration range.\n",
    "\n",
    "Below, we set the FPR `ubound` to 3\\%.\n",
    "\n",
    "Since the average of the Y-axis is now being taken mostly in the region where it is still increasing, notice how the distribution of AUCs shifted to the left (lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%autoreload 2\n",
    "from anomalib.utils.metrics.perimg import AUPImO\n",
    "\n",
    "aupimo = AUPImO(num_thresholds=1000, ubound=0.03)\n",
    "aupimo.cpu()\n",
    "aupimo.update(anomaly_maps, masks)\n",
    "pimoresult, aucs = aupimo.compute()\n",
    "print(f\"{aupimo.ubound=}\")\n",
    "fig, ax = aupimo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `LogPImO`\n",
    "\n",
    "> Pronounced \"log-pee-mo\"\n",
    "\n",
    "Here we show the intuition to another solution for the issue raise in the section \"`AUPImO` with restricted FPR\".\n",
    "\n",
    "The `PImO` curves visually seem to be all *squeezed* on the left (low FPR values), while that's exactly the region where the tradeoff between TP/FP is happening.\n",
    "\n",
    "To better visualize their differences, one can put the X-axis (shared FPR) in log scale -- a `LogPImO` curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg import AUPImO\n",
    "from anomalib.utils.metrics.perimg.plot import _format_axis_rate_metric_log\n",
    "\n",
    "aupimo = AUPImO(num_thresholds=1000)\n",
    "aupimo.cpu()\n",
    "aupimo.update(anomaly_maps, masks)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12), width_ratios=[6, 8])\n",
    "aupimo.plot_all_pimo_curves(ax=axes[0, 1])\n",
    "aupimo.plot(axes=axes[1])\n",
    "_ = axes[0, 0].axis(\"off\")\n",
    "_format_axis_rate_metric_log(axes[0, 1], axis=0)\n",
    "_format_axis_rate_metric_log(axes[1, 1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AULogPImO`\n",
    "\n",
    "> Area Under the Log Per-Image Overlap curve; pronounced \"a-u-log-pee-mo\"\n",
    "\n",
    "We introduce another metric based on the intuition from the section above: the Area Under the `LogPImO` curve.\n",
    "\n",
    "---\n",
    "\n",
    "`LogPImO` curves are just like `PImO`, but they have log(shared FPR) in the X-axis (instead of FPR).\n",
    "\n",
    "The `AULogPImO` primitive score (to be normalized) is\n",
    "\n",
    "$$\n",
    "    \\int_{L}^{U} \\; \\text{TPR}( \\, \\text{FPR} \\, ) \\; d\\text{log(FPR)} \n",
    "    = \\int_{L}^{U} \\; \\text{TPR}( \\, \\text{FPR} \\, ) \\; \\text{FPR}^{-1} \\; d\\text{FPR} \\quad ,\n",
    "    \n",
    "$$\n",
    "\n",
    "where:\n",
    "- $L \\in (0, 1)$: FPR lower bound \n",
    "- $U \\in (0, 1]$: FPR upper bound such that $U > L$\n",
    "- FPR: False Positive Rate\n",
    "- TPR: True Positive Rate\n",
    "\n",
    "The score above (the \"primitive\" score) is normalized to have a score that is $\\in [0, 1]$.\n",
    "\n",
    "The normalization constant is the maximum possible value, when the perfect model perfectly splits scores from normal and anomalous instances ($\\text{TPR} = 1$ for all FPRs):\n",
    "\n",
    "$$\n",
    "    C_{\\text{normaliz.}} \n",
    "    = \\int_{L}^{U} \\;  1 \\; \\cdot \\; \\text{FPR}^{-1} \\; d\\text{FPR}\n",
    "    =  log(U) - log(L) = log(U/L) \\quad ,\n",
    "    \n",
    "$$\n",
    "\n",
    "> **Score of a random model**\n",
    ">\n",
    "> Like `AUROC`, the `AUPImO` score is 50\\% for a random model -- meaning that normal and anomalous scores have the same distribution.\n",
    ">\n",
    "> `AULogPImO` on the other hand assigns a less intuitive score in this situation, which is computed with the following expression:\n",
    "> $$\n",
    "> \\frac{U - L}{log(U / L)}\n",
    "> $$\n",
    "\n",
    "\n",
    "> Important property: the shared FPR (X-axis) only depends on normal images, so the collection of anomalies available to test the model does **not** affect the model requirement and, therefore, the integration range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg import AULogPImO\n",
    "\n",
    "aulogpimo = AULogPImO(num_thresholds=1000, ubound=0.03)\n",
    "print(aulogpimo)\n",
    "print(f\"AULogPImO of a random model: {aulogpimo.random_model_auc:.1%}\")\n",
    "\n",
    "aulogpimo.cpu()\n",
    "aulogpimo.update(anomaly_maps, masks)\n",
    "pimoresult, aucs = aulogpimo.compute()\n",
    "print(sp.stats.describe(aucs[~torch.isnan(aucs)]))  # `~torch.isnan(aucs)` is removing the `nan`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12), width_ratios=[6, 8])\n",
    "aulogpimo.plot_all_logpimo_curves(ax=axes[0, 1])\n",
    "aulogpimo.plot(axes=axes[1])\n",
    "_ = axes[0, 0].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `num_thresholds` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPR: Shared vs. Per-Image\n",
    "\n",
    "The ***shared False Positive Rate (FPR)*** is a central concept. It is the metric used in the x-axis of `PImO`.\n",
    "\n",
    "Its role is to index the metric of interest -- in this case, the Per-Image Overlap (PImO), a.k.a. the in-image True Positive Rate (TPR).\n",
    "\n",
    "In other words, the `PImO` curve is a *function of* the the ***shared FPR*** (the independent metric variable).\n",
    "\n",
    "---\n",
    "\n",
    "Below we visualize how it is built from the FPR of the individual images and show case useful plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.utils.metrics.perimg import PImO\n",
    "\n",
    "pimo = PImO(num_thresholds=1000)\n",
    "pimo.cpu()\n",
    "pimo.update(anomaly_maps, masks)\n",
    "\n",
    "_, fprs, shared_fpr, __, image_classes = pimo.compute()\n",
    "print(f\"{fprs.shape=}\\n{shared_fpr.shape=}\\n{image_classes.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPR: Normal vs. Anomalous Images\n",
    " \n",
    "> Each line bellow corresponds to the FPR in a single image.\n",
    "> \n",
    "> For a given anomaly score threshold, the x-axis value is the mean in-image FPR only considering the normal images (as in `PImO`), and the y-axis is the value in each of the image (one curve per image; both normal and anomalous).\n",
    "\n",
    "Notice how anomalous images tend to have higher FPRs than normal images, especially at low FPR zones -- this is not true for all models.\n",
    "\n",
    "For some models, like PaDiM (used here), this happens (partially) due to score map resizings, which will provoke regions close to the anomaly's border to be classified as anomalous.\n",
    "\n",
    "TODO integrate these notes (comment)\n",
    "<!-- \n",
    "- annotation imperfections in the anomalous images inherent uncertainty in the fpr(anomalous)\n",
    "- choice of operating point: keep it unsupervised by only defining it on normal images\n",
    "- average of operating points interpretation: parametrize the operating point without anomalies!\n",
    "- *[likely]* side effect (**TODO GENERATE EXAMPLE**): lower the FP metric at a given threshold, to FPR vs TPR metric is pushed to the left, increasing the AUC, reducing the difference between images -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg.plot import plot_pimfpr_curves_norm_vs_anom, _format_axis_rate_metric_linear\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "_ = plot_pimfpr_curves_norm_vs_anom(fprs, shared_fpr, image_classes, ax=axes[0])\n",
    "\n",
    "# zoom in the lower left corner (low-FPR region)\n",
    "_ = plot_pimfpr_curves_norm_vs_anom(fprs, shared_fpr, image_classes, ax=axes[1])\n",
    "_format_axis_rate_metric_linear(axes[1], axis=0, lims=(0, 0.10))\n",
    "_format_axis_rate_metric_linear(axes[1], axis=1, lims=(0, 0.10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference of FPR behaviour in normal/anomalous images is also an inherent issue with anomaly annotations, which are not always precise or clearly defined.\n",
    "\n",
    "While using only the normal images to define the x-axis, `PImO` avoids imprecisions in the annotations masks.\n",
    "\n",
    "Besides, this makes the X and Y axes in `PImO` to be completely data-independent: the X-axis only uses normal images, and the Y-axis only uses anomalous images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPR: Per-Image FPR statistics\n",
    "\n",
    "> Each line bellow corresponds to the FPR in a single image.\n",
    "> \n",
    "> For a given anomaly score threshold, the x-axis value is the mean in-image FPR only considering the normal images (as in `PImO`), and the y-axis is the value in each of the image (one curve per image; only normal).\n",
    "\n",
    "Below we show only the FPR curves from normal images and statistics from them.\n",
    "\n",
    "The y-value on a statistic statistic line is computed based on the collection of FPR values at a given (fixed) anomaly score threshold.\n",
    "\n",
    "The `mean` curve is the identity curve because the shared FPR here is defined from that metric.\n",
    "\n",
    "The Standard Error of the Mean (SEM) is computed considering the in-image FPR a random variable normally distributed.\n",
    "\n",
    "---\n",
    "\n",
    "**Weakness (but also strength)**\n",
    "\n",
    "This reveals an inherent weakness of this approach: the x-axis -- which is the reference metric (used to index thresholds) -- naturally shows some uncertainty.\n",
    "\n",
    "However, this will further punish models with more severe worst case scenarios (i.e. with high FPR on normal images). \n",
    "\n",
    "As some images have outlier-high FPR, the average is pushed up, which corresponds to deforming/stretching the `PImO` curves to the right, decreasing the `AUPImO` of all anomalous images.\n",
    "\n",
    "> In the next section, notice from the plot on the right how a single image is stretching the threshold to very high values because their anomaly scores are too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from anomalib.utils.metrics.perimg.plot import plot_pimfpr_curves_norm_only, _format_axis_rate_metric_linear\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "_ = plot_pimfpr_curves_norm_only(fprs, shared_fpr, image_classes, ax=axes[0])\n",
    "\n",
    "# zoom in the lower left corner (low-FPR region)\n",
    "_ = plot_pimfpr_curves_norm_only(fprs, shared_fpr, image_classes, ax=axes[1])\n",
    "_format_axis_rate_metric_linear(axes[1], axis=0, lims=(0, 0.05))\n",
    "_format_axis_rate_metric_linear(axes[1], axis=1, lims=(0, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max per-image FPR (COMING LATER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `AUPImO.ubound` parameter\n",
    "\n",
    "The class parameter `ubound` fixes the upper bound for the shared FPR metric, which corresponds to the lower bound of anomaly score threshold in the range of integration.\n",
    "\n",
    "This upper bound is choosen is interpreted as the \"maximum tolerance of false positives\".\n",
    "\n",
    "The plots below show the per-image FPR vs thresholds (on the right) and vs the shared FPR (on the left), which is there average at each threshold.\n",
    "\n",
    "> Important property: the shared FPR (X-axis) only depends on normal images, so the collection of anomalies available to test the model does **not** affect the model requirement and, therefore, the integration range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from anomalib.utils.metrics.perimg import AUPImO\n",
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg.plot import _format_axis_rate_metric_linear\n",
    "\n",
    "# from matplotlib.ticker import FixedLocator\n",
    "# import numpy as np\n",
    "\n",
    "aupimo = AUPImO(num_thresholds=1000, ubound=0.03)\n",
    "aupimo.cpu()\n",
    "aupimo.update(anomaly_maps, masks)\n",
    "\n",
    "(_, fprs, shared_fpr, __, image_classes), auc = aupimo.compute()\n",
    "print(f\"{fprs.shape=}\\n{shared_fpr.shape=}\\n{image_classes.shape=}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "aupimo.plot_perimg_fprs(axes=axes[0])\n",
    "\n",
    "# zoomed in\n",
    "aupimo.plot_perimg_fprs(axes=axes[1])\n",
    "_format_axis_rate_metric_linear(axes[1, 1], axis=0, lims=(0, 0.05))\n",
    "_format_axis_rate_metric_linear(axes[1, 1], axis=1, lims=(0, 0.05))\n",
    "_format_axis_rate_metric_linear(axes[1, 0], axis=1, lims=(0, 0.05))\n",
    "axes[1, 0].set_xlim(10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `AULogPImO.`{`ubound`, `lbound`} parameters\n",
    "\n",
    "Same as previous section but `AULogPImO` has lower *and* upper FPR bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from anomalib.utils.metrics.perimg import AULogPImO\n",
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg.plot import _format_axis_rate_metric_linear\n",
    "\n",
    "# from matplotlib.ticker import FixedLocator\n",
    "# import numpy as np\n",
    "\n",
    "aulogpimo = AULogPImO(num_thresholds=1000, lbound=0.001, ubound=0.03)\n",
    "aulogpimo.cpu()\n",
    "aulogpimo.update(anomaly_maps, masks)\n",
    "\n",
    "(_, fprs, shared_fpr, __, image_classes), auc = aulogpimo.compute()\n",
    "print(f\"{fprs.shape=}\\n{shared_fpr.shape=}\\n{image_classes.shape=}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "aulogpimo.plot_perimg_fprs(axes=axes[0])\n",
    "\n",
    "# zoomed in\n",
    "aulogpimo.plot_perimg_fprs(axes=axes[1])\n",
    "_format_axis_rate_metric_linear(axes[1, 1], axis=0, lims=(0, 0.03))\n",
    "_format_axis_rate_metric_linear(axes[1, 1], axis=1, lims=(0, 0.03))\n",
    "_format_axis_rate_metric_linear(axes[1, 0], axis=1, lims=(0, 0.03))\n",
    "axes[1, 0].set_xlim(10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrathc/cache (please ignore this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        def get_pred():\n",
    "            \"\"\"\n",
    "            pred (content | nb pixels): (1 | 3), (9 | 2), (90 | 1), (900 | 0)\n",
    "                                     (1 | >= 3), (10 | >= 2), (100 | >=1), (1000 | >= 0)\n",
    "            \"\"\"\n",
    "            pred = torch.ones(1000, dtype=torch.float32)\n",
    "            pred[:100] += 1\n",
    "            pred[:10] += 1\n",
    "            pred[:1] += 1\n",
    "            return pred\n",
    "        \n",
    "        pred_norm = get_pred().reshape(shape)\n",
    "        mask_norm = torch.zeros(1000, dtype=torch.int32).reshape(shape)\n",
    "        expected_fpr = torch.tensor([1, 0.1, 0.01, 0.001])\n",
    "        expected_tpr_norm = torch.full((4,), torch.nan, dtype=torch.float32)\n",
    "\n",
    "        # --- anomalous ---\n",
    "        pred_anom1 = get_pred().reshape(shape) \n",
    "        mask_anom1 = torch.ones(1000, dtype=torch.int32).reshape(shape)\n",
    "        expected_tpr_anom1 = torch.tensor([1, 0.9, 0.99, 0.999])\n",
    "\n",
    "        # only half (500) is anom; out of anom pixels: (1 | 4), (9 | 3), (90 | 2), (400 | 1)\n",
    "        # (10 | >= 3), (100 | >= 2), (500 | >=1), (500 | >= 0)\n",
    "        pred_anom2 = get_pred().reshape(shape) + 1\n",
    "        mask_anom2 = torch.concatenate([torch.ones(500), torch.zeros(500)]).to(torch.int32).reshape(shape)\n",
    "        expected_tpr_anom2 = torch.tensor([1, 1, 0.2, 0.02])\n",
    "\n",
    "        anomaly_maps = torch.stack([pred_norm, pred_anom1, pred_anom2], axis=0)\n",
    "        masks = torch.stack([mask_norm, mask_anom1, mask_anom2], axis=0)\n",
    "        expected_tprs = torch.stack([expected_tpr_norm, expected_tpr_anom1, expected_tpr_anom2], axis=0)\n",
    "        expected_image_classes = torch.tensor([0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del AUPImO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "(CACHE := Path.home() / \".cache\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(anomaly_maps, CACHE / \"anomaly_maps.pt\")\n",
    "torch.save(masks, CACHE / \"masks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "(CACHE := Path.home() / \".cache\").mkdir(exist_ok=True)\n",
    "import torch\n",
    "\n",
    "anomaly_maps = torch.load(CACHE / \"anomaly_maps.pt\")\n",
    "masks = torch.load(CACHE / \"masks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib-dev-gsoc",
   "language": "python",
   "name": "anomalib-dev-gsoc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
