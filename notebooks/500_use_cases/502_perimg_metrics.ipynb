{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Image Metrics\n",
    "\n",
    "How to use, plot, and compare models using per-image [pixel-wise] metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Anomalib\n",
    "\n",
    "The easiest way to install anomalib is to use pip. You can install it from the command line using the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anomalib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a cell print all the outputs instead of just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use MVTec AD DataModule. \n",
    "\n",
    "> See [these notebooks](https://github.com/openvinotoolkit/anomalib/tree/main/notebooks/100_datamodules) for more details on datamodules. \n",
    "\n",
    "We assume that `datasets` directory is created in the `anomalib` root directory and `MVTec` dataset is located in `datasets` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# NOTE: Provide the path to the dataset root directory.\n",
    "#   If the datasets is not downloaded, it will be downloaded to this directory.\n",
    "dataset_root = Path.cwd().parent.parent / \"datasets\" / \"MVTec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working on a segmentation task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.data import TaskType\n",
    "\n",
    "task = TaskType.SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the `hazelnut` category at resolution of 256x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.data.mvtec import MVTec\n",
    "\n",
    "datamodule = MVTec(\n",
    "    root=dataset_root,\n",
    "    category=\"hazelnut\",\n",
    "    image_size=256,\n",
    "    train_batch_size=32,\n",
    "    eval_batch_size=32,\n",
    "    num_workers=8,\n",
    "    task=task,\n",
    ")\n",
    "datamodule.setup()\n",
    "i, data = next(enumerate(datamodule.test_dataloader()))\n",
    "print(f'Image Shape: {data[\"image\"].shape} Mask Shape: {data[\"mask\"].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We will use PaDiM.\n",
    "\n",
    "> See [these notebooks](https://github.com/openvinotoolkit/anomalib/tree/main/notebooks/200_models) for more details on models. \n",
    "\n",
    "The next cell instantiates and trains the model.\n",
    "\n",
    "The `MetricsConfigurationCallback()` will not have metric because they will be created manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from anomalib.utils.callbacks import MetricsConfigurationCallback, PostProcessingConfigurationCallback\n",
    "from anomalib.post_processing import NormalizationMethod, ThresholdMethod\n",
    "from anomalib.models import Padim\n",
    "\n",
    "model = Padim(\n",
    "    input_size=(256, 256),\n",
    "    layers=[\n",
    "        \"layer1\",\n",
    "        \"layer2\",\n",
    "    ],\n",
    "    backbone=\"resnet18\",\n",
    "    pre_trained=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[\n",
    "        PostProcessingConfigurationCallback(\n",
    "            normalization_method=NormalizationMethod.MIN_MAX,\n",
    "            threshold_method=ThresholdMethod.ADAPTIVE,\n",
    "        ),\n",
    "        MetricsConfigurationCallback(),\n",
    "    ],\n",
    "    max_epochs=1,\n",
    "    num_sanity_val_steps=0,  # does not work for padim\n",
    "    accelerator=\"auto\",\n",
    ")\n",
    "\n",
    "trainer.fit(datamodule=datamodule, model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process test images\n",
    "\n",
    "This part is usually happening automatically but here we want to extract the outputs manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "outputs = []\n",
    "for batchidx, batch in enumerate(datamodule.test_dataloader()):\n",
    "    outputs.append(model.test_step_end(model.test_step(batch, batchidx)))\n",
    "\n",
    "anomaly_maps = torch.squeeze(torch.cat([o[\"anomaly_maps\"] for o in outputs], dim=0))\n",
    "masks = torch.squeeze(torch.cat([o[\"mask\"] for o in outputs], dim=0)).int()\n",
    "print(f\"{anomaly_maps.shape=} {masks.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel-wise [set] metrics\n",
    "\n",
    "The usual set pixel-wise metrics. \n",
    "\n",
    "Only one value for the whole test set is measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from anomalib.utils.metrics import AUROC, AUPR\n",
    "\n",
    "metrics = [AUROC(), AUPR()]\n",
    "\n",
    "for metric in metrics:\n",
    "    metric.cpu()\n",
    "    metric.update(anomaly_maps, masks)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"{metric}={metric.compute()}\")\n",
    "    metric.generate_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AUPImO` (init, update, compute)\n",
    "\n",
    "Area Under the Per-Image Overlap (`AUPImO`) \n",
    "\n",
    "Let's instantiate, load the data, then compute PImO curves and their AUCs (AUPImO scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%autoreload 2\n",
    "\n",
    "from anomalib.utils.metrics.perimg import AUPImO\n",
    "\n",
    "aupimo = AUPImO()\n",
    "aupimo.cpu()\n",
    "aupimo.update(anomaly_maps, masks)\n",
    "\n",
    "pimoresult, aucs = aupimo.compute()\n",
    "(thresholds, fprs, shared_fpr, tprs, image_classes) = pimoresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pimoresult?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{thresholds.shape=}\")\n",
    "print(f\"{fprs.shape=}\")\n",
    "print(f\"{shared_fpr.shape=}\")\n",
    "print(f\"{tprs.shape=}\")\n",
    "print(f\"{image_classes.shape=}\")\n",
    "print(f\"{aucs.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `PImO` curves (plot)\n",
    "\n",
    "The PImO curve has a shared X-axis and a per-image Y-axis.\n",
    "\n",
    "The X-axis:\n",
    "- is a metric of False Positives only in the normal images (here it is the set-FPR)\n",
    "- is shared by all image instances\n",
    "\n",
    "The Y-axis: \n",
    "- is the **overlap** between the binary predicted mask and the ground truth mask, which corresponds to the True Positive Rate (TPR) in a single image\n",
    "- has one value per image, so there is one PImO curve per image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aupimo.plot_all_pimo_curves()\n",
    "# TODO add functional interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AUPImO` = AUC(`PImO`)\n",
    "\n",
    "The Area Under the Curve (AUC) is, by consequence, computed for each image, which will be used as is score.\n",
    "\n",
    "Notice that `aucs` has the number of images seen in `outputs` (cf. `masks` below).\n",
    "\n",
    "`aucs` has `nan` values for the normal images because the `Per-Image Overlap`, by definition, is not defined on them (they do not have any positive/anomalous pixels).\n",
    "\n",
    "This is done by design choice so the indexes in `aucs` correspond to the indices of the actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{masks.shape[0]=}  ==  {aucs.shape[0]=}\")\n",
    "print(aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AUPImO` distribuion (boxplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can now analyze the distribution of this True Posivity metric across images and take statistics from the test set (e.g. with `sp.stats.describe`).\n",
    "\n",
    "`AUPImO` has an integrated feature to plot a boxplot from the distribution and inspect representative cases using its statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "print(sp.stats.describe(aucs[~torch.isnan(aucs)]))  # `~torch.isnan(aucs)` is removing the `nan`s\n",
    "aupimo.plot_boxplot()\n",
    "aupimo.boxplot_stats()[-3:]\n",
    "# TODO add functional interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representative samples (curve + boxplot)\n",
    "\n",
    "The two plots (`PImO` curve + `AUPImO` boxplot) are combined with the method `AUPImO.plot()`.\n",
    "\n",
    "The `PImO` curves are plot only for the samples that correspond to the boxplot's statistics (see `AUPImO.boxplot_stats()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aupimo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AUPImO` with restricted FPR (x-axis) range.\n",
    "\n",
    "Look at the curve plots (in linear scale, not log).\n",
    "\n",
    "Most part of the curves are close to `PImO == 1` (i.e. `TPR == 1`) for values of of `FPR >= 30` -- which explains why most of the AUCs are so high.\n",
    "\n",
    "Notice the AUC of a curve with the x-axis in [0, 1] can be interpreted as the average value of the y-axis.\n",
    "So the `AUPImO`, like `AUROC` and `AUPRO` (**TODO ADD REFS**), is a true positive metric averaged over many operating points.\n",
    "\n",
    "On the other hand, the right-most end of the curve coresponds to operating points with high FPR, which is undisarable.\n",
    "\n",
    "Therefore we add a hard restriction to avoid the high levels of FPR and only do the integration of the curve up to an upper bound of FPR level (`fpr_auc_ubound`); the metric is normalized by the size of the integration range so the AUC metric is scaled in [0, 1].\n",
    "\n",
    "---\n",
    "\n",
    "Below, we set the TPR to 3\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from anomalib.utils.metrics.perimg import AUPImO\n",
    "\n",
    "aupimo = AUPImO(num_thresholds=1000, ubound=0.03)\n",
    "aupimo.cpu()\n",
    "aupimo.update(anomaly_maps, masks)\n",
    "pimoresult, aucs = aupimo.compute()\n",
    "print(f\"{aupimo.ubound=}\")\n",
    "fig, ax = aupimo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `LogPImO`\n",
    "\n",
    "Notice (from the boxplot) that most images (~75\\%) have an AUC value above 99\\%, which makes sense from the curves because they all seem quite close to the upper left corner of the plot.\n",
    "\n",
    "To make difference between curves more visible, one can put the X-axis (shared FPR) in log scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg.plot import _format_axis_rate_metric_log\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12), width_ratios=[6, 8])\n",
    "aupimo.plot_all_pimo_curves(ax=axes[0, 1])\n",
    "aupimo.plot(axes=axes[1])\n",
    "axes[0, 0].axis(\"off\")\n",
    "_format_axis_rate_metric_log(axes[0, 1], axis=0)\n",
    "_format_axis_rate_metric_log(axes[1, 1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `num_thresholds` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPR: Shared vs. Per-Image\n",
    "\n",
    "The ***shared False Positive Rate (FPR)*** is a central concept. It is the metric used in the x-axis of `PImO`.\n",
    "\n",
    "Its role is to index the metric of interest -- in this case, the Per-Image Overlap (PImO), a.k.a. the in-image True Positive Rate (TPR).\n",
    "\n",
    "In other words, the `PImO` curve is a *function of* the the ***shared FPR*** (the independent metric variable).\n",
    "\n",
    "---\n",
    "\n",
    "Below we visualize how it is built from the FPR of the individual images and show case useful plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.utils.metrics.perimg import PImO\n",
    "\n",
    "pimo = PImO(num_thresholds=1000, fpr_auc_ubound=0.03)\n",
    "pimo.cpu()\n",
    "pimo.update(anomaly_maps, masks)\n",
    "\n",
    "_, fprs, shared_fpr, __, image_classes = pimo.compute()\n",
    "print(f\"{fprs.shape=}\\n{shared_fpr.shape=}\\n{image_classes.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPR: Normal vs. Anomalous Images\n",
    " \n",
    "> Each line bellow corresponds to the FPR in a single image.\n",
    "> \n",
    "> For a given anomaly score threshold, the x-axis value is the mean in-image FPR only considering the normal images (as in `PImO`), and the y-axis is the value in each of the image (one curve per image; both normal and anomalous).\n",
    "\n",
    "Notice how anomalous images tend to have higher FPRs than normal images, especially at low FPR zones -- this is not true for all models.\n",
    "\n",
    "For some models, like PaDiM (used here), this happens (partially) due to score map resizings, which will provoke regions close to the anomaly's border to be classified as anomalous.\n",
    "\n",
    "TODO integrate these notes (comment)\n",
    "<!-- \n",
    "- annotation imperfections in the anomalous images inherent uncertainty in the fpr(anomalous)\n",
    "- choice of operating point: keep it unsupervised by only defining it on normal images\n",
    "- average of operating points interpretation: parametrize the operating point without anomalies!\n",
    "- *[likely]* side effect (**TODO GENERATE EXAMPLE**): lower the FP metric at a given threshold, to FPR vs TPR metric is pushed to the left, increasing the AUC, reducing the difference between images -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg.plot import plot_pimfpr_curves_norm_vs_anom, _format_axis_rate_metric_linear\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "_ = plot_pimfpr_curves_norm_vs_anom(fprs, shared_fpr, image_classes, ax=axes[0])\n",
    "\n",
    "# zoom in the lower left corner (low-FPR region)\n",
    "_ = plot_pimfpr_curves_norm_vs_anom(fprs, shared_fpr, image_classes, ax=axes[1])\n",
    "_format_axis_rate_metric_linear(axes[1], axis=0, lims=(0, 0.10))\n",
    "_format_axis_rate_metric_linear(axes[1], axis=1, lims=(0, 0.10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference of FPR behaviour in normal/anomalous images is also an inherent issue with anomaly annotations, which are not always precise or clearly defined.\n",
    "\n",
    "While using only the normal images to define the x-axis, `PImO` avoids imprecisions in the annotations masks.\n",
    "\n",
    "Besides, this makes the X and Y axes in `PImO` to be completely data-independent: the X-axis only uses normal images, and the Y-axis only uses anomalous images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPR: Per-Image FPR statistics\n",
    "\n",
    "> Each line bellow corresponds to the FPR in a single image.\n",
    "> \n",
    "> For a given anomaly score threshold, the x-axis value is the mean in-image FPR only considering the normal images (as in `PImO`), and the y-axis is the value in each of the image (one curve per image; only normal).\n",
    "\n",
    "Below we show only the FPR curves from normal images and statistics from them.\n",
    "\n",
    "The y-value on a statistic statistic line is computed based on the collection of FPR values at a given (fixed) anomaly score threshold.\n",
    "\n",
    "The `mean` curve is the identity curve because the shared FPR here is defined from that metric.\n",
    "\n",
    "The Standard Error of the Mean (SEM) is computed considering the in-image FPR a random variable normally distributed.\n",
    "\n",
    "---\n",
    "\n",
    "**Weakness (but also strength)**\n",
    "\n",
    "This reveals an inherent weakness of this approach: the x-axis -- which is the reference metric (used to index thresholds) -- naturally shows some uncertainty.\n",
    "\n",
    "However, this will further punish models with more severe worst case scenarios (i.e. with high FPR on normal images). \n",
    "\n",
    "As some images have outlier-high FPR, the average is pushed up, which corresponds to deforming/stretching the `PImO` curves to the right, decreasing the `AUPImO` of all anomalous images.\n",
    "\n",
    "> In the next section, notice from the plot on the right how a single image is stretching the threshold to very high values because their anomaly scores are too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from anomalib.utils.metrics.perimg.plot import plot_pimfpr_curves_norm_only, _format_axis_rate_metric_linear\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "_ = plot_pimfpr_curves_norm_only(fprs, shared_fpr, image_classes, ax=axes[0])\n",
    "\n",
    "# zoom in the lower left corner (low-FPR region)\n",
    "_ = plot_pimfpr_curves_norm_only(fprs, shared_fpr, image_classes, ax=axes[1])\n",
    "_format_axis_rate_metric_linear(axes[1], axis=0, lims=(0, 0.05))\n",
    "_format_axis_rate_metric_linear(axes[1], axis=1, lims=(0, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max per-image FPR (COMING LATER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fpr_auc_ubound` parameter\n",
    "\n",
    "The class parameter `fpr_auc_ubound` fixes the upper bound for the shared FPR metric, which corresponds to the lower bound of anomaly score threshold in the range of integration.\n",
    "\n",
    "This upper bound is choosen is interpreted as the \"maximum tolerance of false positives\".\n",
    "\n",
    "The plots below show the per-image FPR vs thresholds (on the right) and vs the shared FPR (on the left), which is there average at each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from anomalib.utils.metrics.perimg import AUPImO\n",
    "from matplotlib import pyplot as plt\n",
    "from anomalib.utils.metrics.perimg.plot import _format_axis_rate_metric_linear\n",
    "\n",
    "# from matplotlib.ticker import FixedLocator\n",
    "# import numpy as np\n",
    "\n",
    "aupimo = AUPImO(num_thresholds=1000, ubound=0.03)\n",
    "aupimo.cpu()\n",
    "aupimo.update(anomaly_maps, masks)\n",
    "\n",
    "(_, fprs, shared_fpr, __, image_classes), auc = aupimo.compute()\n",
    "print(f\"{fprs.shape=}\\n{shared_fpr.shape=}\\n{image_classes.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "aupimo.plot_perimg_fprs(axes=axes[0])\n",
    "\n",
    "# zoomed in\n",
    "aupimo.plot_perimg_fprs(axes=axes[1])\n",
    "_format_axis_rate_metric_linear(axes[1, 1], axis=0, lims=(0, 0.05))\n",
    "_format_axis_rate_metric_linear(axes[1, 1], axis=1, lims=(0, 0.05))\n",
    "_format_axis_rate_metric_linear(axes[1, 0], axis=1, lims=(0, 0.05))\n",
    "axes[1, 0].set_xlim(10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrathc/cache (please ignore this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del AUPImO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "(CACHE := Path.home() / \".cache\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(anomaly_maps, CACHE / \"anomaly_maps.pt\")\n",
    "torch.save(masks, CACHE / \"masks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "(CACHE := Path.home() / \".cache\").mkdir(exist_ok=True)\n",
    "import torch\n",
    "\n",
    "anomaly_maps = torch.load(CACHE / \"anomaly_maps.pt\")\n",
    "masks = torch.load(CACHE / \"masks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib-dev-gsoc",
   "language": "python",
   "name": "anomalib-dev-gsoc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
